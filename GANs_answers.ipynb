{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qic-ui1CoF7L"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomasz_bednarski/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-60ca4d6d9d3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Just some imports\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XdRUB5WKquE3"
   },
   "source": [
    "# MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_1ACI7mr74H"
   },
   "source": [
    "MNIST is a large database of handwritten digits that is commonly used for training various image processing systems.\n",
    "\n",
    "The MNIST database consists of 70000 grayscale digits, where each digit is an image with a size of 28x28 pixels.\n",
    "\n",
    "The datased is splitted into three subsets:\n",
    "\n",
    "\n",
    "1. Train set -- 55k images\n",
    "2. Test set -- 10k images\n",
    "3. Validation set -- 5k images\n",
    "\n",
    "\n",
    "TensorFlow package has already built-in functions to deal with the MNIST dataset. Below one can find how to use it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "r8ULK8gSrLxc"
   },
   "outputs": [],
   "source": [
    "# Function for plotting the MNIST images\n",
    "def plot(image):\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image.reshape(28,28), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 156,
     "output_extras": [
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1627,
     "status": "ok",
     "timestamp": 1515434286745,
     "user": {
      "displayName": "Jakub Chłędowski",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "105861283762913110918"
     },
     "user_tz": -60
    },
    "id": "g816Ub6ZpCCI",
    "outputId": "28264c1e-cfd4-47f3-929d-02d338479988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q2b_1O7awdZm"
   },
   "source": [
    "Now we can easily deal with the MNIST data, by using:\n",
    "\n",
    "\n",
    "1. mnist.train.images\n",
    "2. mnist.test.images\n",
    "3. mnist.validation.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 156,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 940,
     "status": "ok",
     "timestamp": 1515434291566,
     "user": {
      "displayName": "Jakub Chłędowski",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "105861283762913110918"
     },
     "user_tz": -60
    },
    "id": "-M8pQvH5rO7z",
    "outputId": "3c2c745f-70ca-4e55-faa4-e72bc73f3e16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGES:\n",
      "(55000, 784)\n",
      "(10000, 784)\n",
      "(5000, 784)\n",
      "LABELS:\n",
      "(55000, 10)\n",
      "(10000, 10)\n",
      "(5000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of images from each subset\n",
    "print (\"IMAGES:\")\n",
    "print (mnist.train.images.shape)\n",
    "print (mnist.test.images.shape)\n",
    "print (mnist.validation.images.shape)\n",
    "\n",
    "# Print the shapes of labels from each subset\n",
    "print (\"LABELS:\")\n",
    "print (mnist.train.labels.shape)\n",
    "print (mnist.test.labels.shape)\n",
    "print (mnist.validation.labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dolBLdWcWmqG"
   },
   "source": [
    "As we can see, every image is now a 784 dimensional vector, as by default it is reshaped from the matrix of a size 28x28 pixels.\n",
    "\n",
    "Also we see that labels are 10 dimensional vectors, what is caused by the fact that they are in one-hot-encoding form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2738,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1069,
     "status": "ok",
     "timestamp": 1515434294467,
     "user": {
      "displayName": "Jakub Chłędowski",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "105861283762913110918"
     },
     "user_tz": -60
    },
    "id": "ZeUmb24leShM",
    "outputId": "34e13709-66bd-4351-a40e-a8960628b497"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.38039219,  0.37647063,  0.3019608 ,\n",
       "        0.46274513,  0.2392157 ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.35294119,  0.5411765 ,  0.92156869,\n",
       "        0.92156869,  0.92156869,  0.92156869,  0.92156869,  0.92156869,\n",
       "        0.98431379,  0.98431379,  0.97254908,  0.99607849,  0.96078438,\n",
       "        0.92156869,  0.74509805,  0.08235294,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.54901963,\n",
       "        0.98431379,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.74117649,  0.09019608,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.88627458,  0.99607849,  0.81568635,\n",
       "        0.78039223,  0.78039223,  0.78039223,  0.78039223,  0.54509807,\n",
       "        0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,\n",
       "        0.50196081,  0.8705883 ,  0.99607849,  0.99607849,  0.74117649,\n",
       "        0.08235294,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.14901961,  0.32156864,  0.0509804 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.13333334,\n",
       "        0.83529419,  0.99607849,  0.99607849,  0.45098042,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.32941177,  0.99607849,\n",
       "        0.99607849,  0.91764712,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.32941177,  0.99607849,  0.99607849,  0.91764712,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.41568631,  0.6156863 ,\n",
       "        0.99607849,  0.99607849,  0.95294124,  0.20000002,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.09803922,  0.45882356,  0.89411771,  0.89411771,\n",
       "        0.89411771,  0.99215692,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.94117653,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.26666668,  0.4666667 ,  0.86274517,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.55686277,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.14509805,  0.73333335,\n",
       "        0.99215692,  0.99607849,  0.99607849,  0.99607849,  0.87450987,\n",
       "        0.80784321,  0.80784321,  0.29411766,  0.26666668,  0.84313732,\n",
       "        0.99607849,  0.99607849,  0.45882356,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.44313729,  0.8588236 ,  0.99607849,  0.94901967,  0.89019614,\n",
       "        0.45098042,  0.34901962,  0.12156864,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.7843138 ,  0.99607849,  0.9450981 ,\n",
       "        0.16078432,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.66274512,  0.99607849,\n",
       "        0.6901961 ,  0.24313727,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.18823531,\n",
       "        0.90588242,  0.99607849,  0.91764712,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.07058824,  0.48627454,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.32941177,  0.99607849,  0.99607849,\n",
       "        0.65098041,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.54509807,  0.99607849,  0.9333334 ,  0.22352943,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.82352948,  0.98039222,  0.99607849,\n",
       "        0.65882355,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.94901967,  0.99607849,  0.93725497,  0.22352943,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.34901962,  0.98431379,  0.9450981 ,\n",
       "        0.33725491,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.01960784,\n",
       "        0.80784321,  0.96470594,  0.6156863 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.01568628,  0.45882356,  0.27058825,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the vector of sample training image\n",
    "mnist.train.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 265,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 903,
     "status": "ok",
     "timestamp": 1515259009945,
     "user": {
      "displayName": "Łukasz Maziarka",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "111922214428585581485"
     },
     "user_tz": -60
    },
    "id": "MhUzPQoawZcw",
    "outputId": "fa8d8c6b-8f99-4fb3-d226-65724d07f133"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f208b53d790>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the sample training image\n",
    "plot(mnist.train.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1023,
     "status": "ok",
     "timestamp": 1515259014833,
     "user": {
      "displayName": "Łukasz Maziarka",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "111922214428585581485"
     },
     "user_tz": -60
    },
    "id": "fgn9mdisZHUM",
    "outputId": "47883fd4-e076-41ff-8b8a-f7a34e08bdbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the label of sample training image\n",
    "mnist.train.labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o4958-YRqSI-"
   },
   "source": [
    "# Convolutional Neural Network for MNIST classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XeJkuR15VDj_"
   },
   "source": [
    "## Building the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "reHRRPUHk7FV"
   },
   "source": [
    "We will create the CNN for digits classification, with the following architecture:\n",
    "\n",
    "\n",
    "1.   First convolutional layer, that maps one grayscale image to 32 feature maps.\n",
    "2.   Second convolutional layer, that maps 32 feature maps to 64 feature maps.\n",
    "3.   Fully connected layer 1, that maps our 64 feature maps into one layer, with 1024 features.\n",
    "4.   Fully connected layer 2, that maps the 1024 features to 10 classes, one for each digit\n",
    "\n",
    "We will use filters with width and height equal to 5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zvfwSRadongm"
   },
   "source": [
    "#### At the beggining we have to create some auxiliary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "atNG8IgVo5B9"
   },
   "outputs": [],
   "source": [
    "def get_weight_variable(shape):\n",
    "  \"\"\"\n",
    "  Write a function, that will return the tf.Variable of specified shape, with\n",
    "  coefficients initialized by random, sampled from a normal distribution with\n",
    "  mean = 0 and sd = 0.02\n",
    "  \"\"\"\n",
    "  init = tf.random_normal(shape, stddev=0.02)\n",
    "  return tf.Variable(init)\n",
    "#  pass\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "  \"\"\"\n",
    "  Write a function, that will return the result of a convolution between\n",
    "  a tensor x and a weight vector W. We recommend using strides equal to 1\n",
    "  in every direction and use SAME padding.\n",
    "  \"\"\"\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') # 4 dim to batch\n",
    "#  pass\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  \"\"\"\n",
    "  Write a function, that will return the result of max pooling operation done\n",
    "  on a tensor x, that will reduce the size of inner image. \n",
    "  The length and width of a pooling layer window should be equal to 2.\n",
    "  \"\"\"\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], \n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "#   pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1LJASUEUUZK1"
   },
   "source": [
    "#### Create placeholders for training data, remember about a propper shape for training images (in mnist.train.images every digit is a 784D vector) and labels (In training dataset labels are in one-hot-encoding form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "z6ItZS1rpCOe"
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "#x = \n",
    "#y = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eEBF4aHIXxxa"
   },
   "source": [
    "#### Reshape the x vector into a rank 4 tensor with shapes: [batch_size, rows, columnss, colors/filters]. Keep in mind, that we should have 28x28 image, with only one color (as the image is in grayscale) and that batch size will be given later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "LCVImfFordmU"
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, 28, 28, 1]) # [b_size, rows, cols, colors/filters]\n",
    "#x_image = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yy-6MB2rkMMb"
   },
   "source": [
    "#### Create first convolutional layer, that will map one grayscale image into 32 feature maps. To do so, we will use 32 convolutional filters with sizes 5x5x1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "HmWTYyK3rdhv"
   },
   "outputs": [],
   "source": [
    "\"\"\" Initialize weights matrix W \"\"\"\n",
    "W_conv1 = get_weight_variable([5, 5, 1, 32]) # 32 filters with size 5x5x1  # \n",
    "#W_conv1 = # 32 filters with size 5x5x1\n",
    "\n",
    "\"\"\" Initialize biases vector \"\"\"\n",
    "b_conv1 = get_weight_variable([32])\n",
    "#b_conv1 = # 32 filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "WoG5Q-Syrdcl"
   },
   "outputs": [],
   "source": [
    "\"\"\" Apply convolution operation between image and weights, then add bias and apply relu function \"\"\"\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # [batch_size, 28, 28, 32]   # relu - dodanie formy nieliniowosci\n",
    "#h_conv1 = # [batch_size, 28, 28, 32]\n",
    "\n",
    "\"\"\" Apply max pooling operation on h_conv1 \"\"\"\n",
    "h_pool1 = max_pool_2x2(h_conv1) # [batch_size, 14, 14, 32]\n",
    "#h_pool1 = # [batch_size, 14, 14, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3AKLlmbVxXpj"
   },
   "source": [
    "#### Create second convolutional layer, that will map resulted 32 feature maps into 64 features maps. To do so, we will use 64 convolutional filters with sizes 5x5x32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "BBxMYoFVrdWb"
   },
   "outputs": [],
   "source": [
    "\"\"\" Initialize weights matrix W \"\"\"\n",
    "W_conv2 = get_weight_variable([5, 5, 32, 64]) # 64 filters with size 5x5x32\n",
    "#W_conv2 =  # 64 filters with size 5x5x32\n",
    "\n",
    "\"\"\" Initialize biases vector b \"\"\"\n",
    "b_conv2 = get_weight_variable([64])\n",
    "#b_conv2 = # 64 filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "C2yd_IGaxlgj"
   },
   "outputs": [],
   "source": [
    "\"\"\" Apply convolution operation between image and weights, then add bias and apply relu function \"\"\"\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) # [batch_size, 14, 14, 64] # relu adding non-linearity\n",
    "#h_conv2 = \n",
    "\n",
    "\"\"\" Apply max pooling operation on h_conv1 \"\"\"\n",
    "h_pool2 = max_pool_2x2(h_conv2) # [batch_size, 7, 7, 64]\n",
    "#h_pool2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A7qHA40KyOaD"
   },
   "source": [
    "#### Create first fully connected layer -- after 2 rounds of downsampling, our 28x28 image is down to 7x7x64 feature maps -- now map this to 1024 features, with using of fully connected layer, with ReLU activation function and dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Wht3fwJxyPB6"
   },
   "outputs": [],
   "source": [
    "\"\"\" Initialize weights matrix W, which maps 7x7x64 feature maps into one layer with 1024 features \"\"\"\n",
    "W_fc1 = get_weight_variable([7 * 7 * 64, 1024])\n",
    "#W_fc1 = # 7x7x64 -> 1024\n",
    "\n",
    "\"\"\" Initialize biases vector b \"\"\"\n",
    "b_fc1 = get_weight_variable([1024])\n",
    "#b_fc1 = # 1024 weights\n",
    "\n",
    "\"\"\" Reshape the result from the last convolutional layer from [batch_size, 7, 7, 64] to [batch_size, 7*7*64],\n",
    "    as this is the shape that is expected by the weight matrix W_fc1. \"\"\"\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "#h_pool2_flat = # 7*7*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "43QWuAELTEin"
   },
   "outputs": [],
   "source": [
    "\"\"\" Apply matrix multiplication between image and weights, then add bias and apply relu function \"\"\"\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "#h_fc1 = # [batch_size, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "LjM4vDnVUJYx"
   },
   "outputs": [],
   "source": [
    "\"\"\" Create placeholder for the dropout probability \"\"\"\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "#keep_prob =\n",
    "\n",
    "\"\"\" Apply dropout with probability = keep_prob \"\"\"\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "#h_fc1_drop = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5pwl8o9EU_PS"
   },
   "source": [
    "#### Create the second convolutional layer, that maps 1024 features from the last layer into 10 classes, one for each digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TQhjbZ6bWcmH"
   },
   "source": [
    "The softmax function applied on the result of this layer *y_conv* will give us the probabilities for every class that our convolutional neural network gives for the given image. Although we won't use this function at the moment, as this could be numerically unstable. We will handle this problem during the training step. At the moment we want to keep only logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "N87axhCtU-s8"
   },
   "outputs": [],
   "source": [
    "\"\"\" Initialize weights matrix W, which maps 1024 features into one layer with 10 features \"\"\"\n",
    "W_fc2 = get_weight_variable([1024, 10])\n",
    "#W_fc2 = # 1024 -> 10\n",
    "\n",
    "\"\"\" Initialize biases vector b \"\"\"\n",
    "b_fc2 = get_weight_variable([10])\n",
    "#b_fc2 = # 10 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "hE4BywxYVrKf"
   },
   "outputs": [],
   "source": [
    "\"\"\" Apply matrix multiplication between image and weights, then add bias to get logits \"\"\"\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2 \n",
    "#y_conv = # [batch_size, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OzkjDw7wXil8"
   },
   "source": [
    "## Network training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j80E1kqQXtqT"
   },
   "source": [
    "We will now try to train our network. For this purpose we have to define the loss function, cross entropy in our example and optimizer, in our example we will use Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RyuwoGqXYWgN"
   },
   "source": [
    "#### Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "wqpeZX68Xovk"
   },
   "outputs": [],
   "source": [
    "\"\"\" Define the cross entropy loss function\n",
    "    Remember that in the return from our network - 'y_conv', we didn't use the softmax function to overcome problems\n",
    "    with numerical stability, that's why we should use now the function named softmax_cross_entropy_with_logits, that\n",
    "    is more numerical stable. \"\"\"\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_conv)\n",
    "#cross_entropy = ###\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "hlqJyw1oYl8C"
   },
   "outputs": [],
   "source": [
    "\"\"\" Define the Adam optimizer with parameter equal to 1e-4, that will minimize our cross_entropy loss function \"\"\"\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "#train_step ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YIjNNYacZ9Qv"
   },
   "source": [
    "#### Check whether our network returns correct predictions and calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "cDdVlV8IZ9bo"
   },
   "outputs": [],
   "source": [
    "\"\"\" Create a vector that tells us, whether the predictions from our net - y_conv\n",
    "    are equal to the correct digit labels - y. \"\"\"\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y, 1))\n",
    "# correct_prediction = ###\n",
    "correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "\n",
    "\"\"\" Calculate the accurracy of correct predictions \"\"\"\n",
    "accuracy = tf.reduce_mean(correct_prediction)\n",
    "#accuracy = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_yDuzfqcfRk"
   },
   "source": [
    "#### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOjWtD2Scm8R"
   },
   "source": [
    "The following code will train our network with using of our predefined Adam optimizer, based on the batch size equal to 64 and with 20000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 929,
     "output_extras": [
      {
       "item_id": 51
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1176122,
     "status": "ok",
     "timestamp": 1515260414238,
     "user": {
      "displayName": "Łukasz Maziarka",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "111922214428585581485"
     },
     "user_tz": -60
    },
    "id": "TLYyfcX-cfb8",
    "outputId": "1532cda8-fba7-44d8-bccd-3726f4b03a3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, validation accuracy: 0.09799999743700027\n",
      "step: 100, validation accuracy: 0.7950000166893005\n",
      "step: 200, validation accuracy: 0.8849999904632568\n",
      "step: 300, validation accuracy: 0.9150000214576721\n",
      "step: 400, validation accuracy: 0.9290000200271606\n",
      "step: 500, validation accuracy: 0.9359999895095825\n",
      "step: 600, validation accuracy: 0.9419999718666077\n",
      "step: 700, validation accuracy: 0.9509999752044678\n",
      "step: 800, validation accuracy: 0.9490000009536743\n",
      "step: 900, validation accuracy: 0.9589999914169312\n",
      "step: 1000, validation accuracy: 0.9620000123977661\n",
      "step: 1100, validation accuracy: 0.9660000205039978\n",
      "step: 1200, validation accuracy: 0.9649999737739563\n",
      "step: 1300, validation accuracy: 0.9670000076293945\n",
      "step: 1400, validation accuracy: 0.9679999947547913\n",
      "step: 1500, validation accuracy: 0.9739999771118164\n",
      "step: 1600, validation accuracy: 0.9750000238418579\n",
      "step: 1700, validation accuracy: 0.9769999980926514\n",
      "step: 1800, validation accuracy: 0.9760000109672546\n",
      "step: 1900, validation accuracy: 0.9769999980926514\n",
      "step: 2000, validation accuracy: 0.9769999980926514\n",
      "step: 2100, validation accuracy: 0.9779999852180481\n",
      "step: 2200, validation accuracy: 0.9789999723434448\n",
      "step: 2300, validation accuracy: 0.9789999723434448\n",
      "step: 2400, validation accuracy: 0.9800000190734863\n",
      "step: 2500, validation accuracy: 0.9800000190734863\n",
      "step: 2600, validation accuracy: 0.9819999933242798\n",
      "step: 2700, validation accuracy: 0.9819999933242798\n",
      "step: 2800, validation accuracy: 0.9829999804496765\n",
      "step: 2900, validation accuracy: 0.9810000061988831\n",
      "step: 3000, validation accuracy: 0.9850000143051147\n",
      "step: 3100, validation accuracy: 0.9850000143051147\n",
      "step: 3200, validation accuracy: 0.9829999804496765\n",
      "step: 3300, validation accuracy: 0.9829999804496765\n",
      "step: 3400, validation accuracy: 0.9850000143051147\n",
      "step: 3500, validation accuracy: 0.9860000014305115\n",
      "step: 3600, validation accuracy: 0.9860000014305115\n",
      "step: 3700, validation accuracy: 0.984000027179718\n",
      "step: 3800, validation accuracy: 0.9850000143051147\n",
      "step: 3900, validation accuracy: 0.9869999885559082\n",
      "step: 4000, validation accuracy: 0.9869999885559082\n",
      "step: 4100, validation accuracy: 0.9869999885559082\n",
      "step: 4200, validation accuracy: 0.9869999885559082\n",
      "step: 4300, validation accuracy: 0.9860000014305115\n",
      "step: 4400, validation accuracy: 0.9879999756813049\n",
      "step: 4500, validation accuracy: 0.9879999756813049\n",
      "step: 4600, validation accuracy: 0.9869999885559082\n",
      "step: 4700, validation accuracy: 0.9879999756813049\n",
      "step: 4800, validation accuracy: 0.9879999756813049\n",
      "step: 4900, validation accuracy: 0.9860000014305115\n",
      "step: 5000, validation accuracy: 0.9890000224113464\n",
      "step: 5100, validation accuracy: 0.9900000095367432\n",
      "step: 5200, validation accuracy: 0.9890000224113464\n",
      "step: 5300, validation accuracy: 0.9879999756813049\n",
      "step: 5400, validation accuracy: 0.9900000095367432\n",
      "step: 5500, validation accuracy: 0.9890000224113464\n",
      "step: 5600, validation accuracy: 0.9890000224113464\n",
      "step: 5700, validation accuracy: 0.9890000224113464\n",
      "step: 5800, validation accuracy: 0.9879999756813049\n",
      "step: 5900, validation accuracy: 0.9900000095367432\n",
      "step: 6000, validation accuracy: 0.9900000095367432\n",
      "step: 6100, validation accuracy: 0.9900000095367432\n",
      "step: 6200, validation accuracy: 0.9900000095367432\n",
      "step: 6300, validation accuracy: 0.9900000095367432\n",
      "step: 6400, validation accuracy: 0.9900000095367432\n",
      "step: 6500, validation accuracy: 0.9900000095367432\n",
      "step: 6600, validation accuracy: 0.9900000095367432\n",
      "step: 6700, validation accuracy: 0.9900000095367432\n",
      "step: 6800, validation accuracy: 0.9909999966621399\n",
      "step: 6900, validation accuracy: 0.9890000224113464\n",
      "step: 7000, validation accuracy: 0.9900000095367432\n",
      "step: 7100, validation accuracy: 0.9900000095367432\n",
      "step: 7200, validation accuracy: 0.9909999966621399\n",
      "step: 7300, validation accuracy: 0.9909999966621399\n",
      "step: 7400, validation accuracy: 0.9890000224113464\n",
      "step: 7500, validation accuracy: 0.9919999837875366\n",
      "step: 7600, validation accuracy: 0.9909999966621399\n",
      "step: 7700, validation accuracy: 0.9909999966621399\n",
      "step: 7800, validation accuracy: 0.9900000095367432\n",
      "step: 7900, validation accuracy: 0.9900000095367432\n",
      "step: 8000, validation accuracy: 0.9900000095367432\n",
      "step: 8100, validation accuracy: 0.9919999837875366\n",
      "step: 8200, validation accuracy: 0.9909999966621399\n",
      "step: 8300, validation accuracy: 0.9909999966621399\n",
      "step: 8400, validation accuracy: 0.9919999837875366\n",
      "step: 8500, validation accuracy: 0.9919999837875366\n",
      "step: 8600, validation accuracy: 0.9919999837875366\n",
      "step: 8700, validation accuracy: 0.9900000095367432\n",
      "step: 8800, validation accuracy: 0.9919999837875366\n",
      "step: 8900, validation accuracy: 0.9919999837875366\n",
      "step: 9000, validation accuracy: 0.9919999837875366\n",
      "step: 9100, validation accuracy: 0.9900000095367432\n",
      "step: 9200, validation accuracy: 0.9919999837875366\n",
      "step: 9300, validation accuracy: 0.9900000095367432\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a2ace2fa4914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Fill data into placeholders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Print the validation accuracy every 100 steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   2040\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2041\u001b[0m     \"\"\"\n\u001b[0;32m-> 2042\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m \u001b[0m_gradient_registry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegistry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gradient\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4488\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4489\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4490\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  for i in range(2000):\n",
    "    # Get the batch with 64 images from the MNIST training set\n",
    "    batch = mnist.train.next_batch(64)\n",
    "    \n",
    "    # Fill data into placeholders\n",
    "    train_step.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\n",
    "    \n",
    "    # Print the validation accuracy every 100 steps\n",
    "    if i % 100 == 0:\n",
    "      validation_accuracy = accuracy.eval(feed_dict={\n",
    "          x: mnist.validation.images, y: mnist.validation.labels, keep_prob: 1.0})\n",
    "      print('step: {}, validation accuracy: {}'.format(i, round(validation_accuracy,3)))\n",
    "    \n",
    "\n",
    "  # Print the test set accuracy\n",
    "  print('test accuracy: {}'.format(round(accuracy.eval(feed_dict={\n",
    "      x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0}),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFgS_p-htlLD"
   },
   "source": [
    "# Generative adversarial networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4iaPYy_ptqCb"
   },
   "source": [
    "Our GANs will be composed of two neural networks: Discriminator and Generator. The role of the discriminator is to classify whether the given image is real or generated by generator and the role of the generator is to generate image good enough to cheat the discriminator.\n",
    "\n",
    "To train GANs we will use the MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SneHzU0c_XSO"
   },
   "source": [
    "## Create Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "700uQItYB6YY"
   },
   "source": [
    "We will use the convolutional neural network, as the discriminator. We can reuse some parts of code that we already have.\n",
    "\n",
    "\n",
    "Our discriminator network will have the following architecture:\n",
    "\n",
    "1.  First convolutional layer, that maps one grayscale image to 16 feature maps.\n",
    "2.  Second convolutional layer, that maps 16 feature maps to 64 feature maps.\n",
    "3.  Third convolutional layer, that maps 64 feature maps to 256 feature maps.\n",
    "4.  Fully connected layer, that maps our 256 feature maps into output layer, with 2 classes (one for real and one for generated images).\n",
    "\n",
    "In our network we won't use the max pooling operation - instead we will use the stride of size 2 in the width and height directions, what will divide them in the input image by two.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7GjVPlJoJReg"
   },
   "source": [
    "#### Initialize all weights and biases that will be used by our discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "YI07hdSRv-tf"
   },
   "outputs": [],
   "source": [
    "\"\"\" Initialize weights matrix W and biases vector b, that will be used by \n",
    "    the first convolutional layer -> 16 filters with size 5x5x1 \"\"\"\n",
    "# d_W1 = get_weight_variable([5, 5, 1, 16]) # 16 filters with size 5x5x1\n",
    "# d_b1 = get_weight_variable([16]) \n",
    "d_W1 = # 16 filters with size 5x5x1\n",
    "d_b1 = \n",
    "\n",
    "\n",
    "\"\"\" Initialize weights matrix W and biases vector b, that will be used by \n",
    "    the second convolutional layer -> 64 filters with size 5x5x16 \"\"\"\n",
    "# d_W2 = get_weight_variable([5, 5, 16, 64]) # 64 filters with size 5x5x16 # 16 liczba filtrow w poprz kroku sieci (filtry wrzucamy w 3-ci wymiar)\n",
    "# d_b2 = get_weight_variable([64])\n",
    "d_W2 = # 64 filters with size 5x5x16\n",
    "d_b2 = \n",
    "\n",
    "\n",
    "\"\"\" Initialize weights matrix W and biases vector b, that will be used by \n",
    "    the third convolutional layer -> 256 filters with size 5x5x64 \"\"\"\n",
    "# d_W3 = get_weight_variable([5, 5, 64, 256]) # 256 filters with size 5x5x64\n",
    "# d_b3 = get_weight_variable([256])\n",
    "d_W3 = # 256 filters with size 5x5x64\n",
    "d_b3 = \n",
    "\n",
    "\n",
    "\"\"\" Initialize weights matrix W and biases vector b, that will be used by \n",
    "    the output fully connected layer -> from 4096 features to 2 output classes \"\"\"\n",
    "# d_W4 = get_weight_variable([4096, 1]) # from 4096 to 1 feature\n",
    "# d_b4 = get_weight_variable([1])\n",
    "d_W4 = # from 4096 to 1 feature\n",
    "d_b4 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ELmSU34LlYP"
   },
   "source": [
    "#### Create the discriminator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "D2y91iSALk1K"
   },
   "outputs": [],
   "source": [
    "def discriminator(x):\n",
    "  \n",
    "  \"\"\" Create first convolutional layer - remember that we use stride 2 in the width and height directions,\n",
    "      what divide them in the input image by two, please use SAME padding.\n",
    "      Apply convolution operation and then ReLU function\n",
    "      Dyskryminator to siec konwolucyjna podobna jak byla powyzej, stride 2\n",
    "      \"\"\"\n",
    "#   x = tf.nn.conv2d(x, d_W1, [1, 2, 2, 1], padding='SAME') # [batch_size, 14, 14, 16]  \n",
    "#   x = tf.nn.relu(x + d_b1) \n",
    "  x = # [batch_size, 14, 14, 16]\n",
    "  x = \n",
    "  \n",
    "  \n",
    "  \"\"\" Create second convolutional layer - remember that we use stride 2 in the width and height directions,\n",
    "      what divide them in the input image by two, please use SAME padding.\n",
    "      Apply convolution operation and then ReLU function\"\"\"\n",
    "#   x = tf.nn.conv2d(x, d_W2, [1, 2, 2, 1], padding='SAME') # [batch_size, 7, 7, 64]\n",
    "#   x = tf.nn.relu(x + d_b2)\n",
    "  x = # [batch_size, 7, 7, 64]\n",
    "  x = \n",
    "  \n",
    "  \n",
    "  \"\"\" Create third convolutional layer - remember that we use stride 2 in the width and height directions,\n",
    "      what divide them in the input image by two, please use SAME padding.\n",
    "      Apply convolution operation and then ReLU function\"\"\"\n",
    "#   x = tf.nn.conv2d(x, d_W3, [1, 2, 2, 1], padding='SAME') # [batch_size, 4, 4, 256]\n",
    "#   x = tf.nn.relu(x + d_b3) \n",
    "  x = # [batch_size, 4, 4, 256]\n",
    "  x = \n",
    "  \n",
    "  \n",
    "  \"\"\" Reshape the input, into 4*4*256 = 4096 dimensions, so as we could use it in the last layer of our network \"\"\"\n",
    "#   x = tf.reshape(x, [-1, 4*4*256]) \n",
    "  x = \n",
    "\n",
    "  \n",
    "  \"\"\" Create fully connected layer of our network, multiply the x by d_W4 and add the proper batch,\n",
    "      this will give us the logits of our discriminator network \"\"\"\n",
    "#   D_logit = tf.matmul(x, d_W4) + d_b4 # Fully connected layer from [b_size, 4096] to [b_size, 2]\n",
    "  D_logit = # Fully connected layer from [b_size, 4096] to [b_size, 2]\n",
    "  \n",
    "  \n",
    "  \"\"\" Create also probabilities returned by our network, just apply the softmax function on logits \"\"\"\n",
    "#   D_prob = tf.nn.sigmoid(D_logit)\n",
    "  D_prob = \n",
    "\n",
    "  return D_prob, D_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "anrVSE3VQwR0"
   },
   "source": [
    "## Create Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "21UPMYCHRAFH"
   },
   "source": [
    "Generator network will create the 28x28 image, from the initial 100D vector, sampled from a normal distribution, with using of the transpose convolution operation (https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d).\n",
    "\n",
    "\n",
    "Our generator network will have the following architecture:\n",
    "\n",
    "\n",
    "1.  Fully connected layer, that maps our initial random 100 dimensions into 4096 features (4x4 images with 256 channels).\n",
    "2.  First transpose convolutional layer, that maps 4x4x256 images into 7x7 images with 64 channels.\n",
    "3.  Second transpose convolutional layer, that maps 7x7x64 images into 14x14 images with 16 channels.\n",
    "4.  Final transpose convolutional layer, that maps 14x14x16 images into 28x28 grayscale images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lY4Ml4EjTIHa"
   },
   "source": [
    "#### Initialize all weights and biases that will be used by our generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KFY883iFRAaB"
   },
   "outputs": [],
   "source": [
    "\"\"\" Initialize weights matrix W and biases vector b, that will be used by \n",
    "    the input fully connected layer -> from 100 random generated features into 4096 dimentions,\n",
    "    which after reshaping will give us the 4x4x256 image \"\"\"\n",
    "# g_W1 = get_weight_variable([100, 4096]) # from 100 to 4096 features\n",
    "# g_b1 = get_weight_variable([4096])\n",
    "g_W1 = # from 100 to 4096 features\n",
    "g_b1 = \n",
    "\n",
    "\n",
    "\"\"\" Initialize weights matrix W and biases vector b, that will be used by \n",
    "    the first transpose convolutional layer -> 64 transpose_conv filters with size 5x5x256 \"\"\"\n",
    "# g_W2 = get_weight_variable([5, 5, 64, 256]) # 64 transpose_conv filters with size 5x5x256\n",
    "# g_b2 = get_weight_variable([64])\n",
    "g_W2 = # 64 transpose_conv filters with size 5x5x256\n",
    "g_b2 = \n",
    "\n",
    "\n",
    "\"\"\" Initialize weights matrix W and biases vector b, that will be used by \n",
    "    the second transpose convolutional layer -> 16 transpose_conv filters with size 5x5x64 \"\"\"\n",
    "# g_W3 = get_weight_variable([5, 5, 16, 64]) # 16 transpose_conv filters with size 5x5x64\n",
    "# g_b3 = get_weight_variable([16])\n",
    "g_W3 = # 16 transpose_conv filters with size 5x5x64\n",
    "g_b3 =\n",
    "\n",
    "\n",
    "\"\"\" Initialize weights matrix W and biases vector b, that will be used by \n",
    "    the third transpose convolutional layer -> 1 transpose_conv filter with size 5x5x16 \"\"\"\n",
    "# g_W4 = get_weight_variable([5, 5, 1, 16]) # 1 transpose_conv filter with size 5x5x16\n",
    "# g_b4 = get_weight_variable([1])\n",
    "g_W4 = # 1 transpose_conv filter with size 5x5x16\n",
    "g_b4 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gOegwDSQVTvP"
   },
   "source": [
    "#### Create the generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "fFW1bv9lVXIf"
   },
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "\n",
    "  # Fully connected layer from [b_size, 100] to [b_size, 4096]\n",
    "#   z = tf.matmul(z, g_W1) + g_b1\n",
    "#   z = tf.nn.relu(z)\n",
    "#   z = tf.nn.dropout(z, g_keep_prob)\n",
    "#   z = tf.reshape(z, [-1, 4, 4, 256]) # reshape from [b_size, 4096] to [b_size, 4, 4, 256]\n",
    "  z = \n",
    "  z = # reshape from [b_size, 4096] to [b_size, 4, 4, 256]\n",
    "\n",
    "  \n",
    "  \"\"\" Create first transpose convolutional layer, please use SAME padding.\n",
    "      This layer will almost double the size of width and height of our image - \n",
    "      instead of 4x4x256, the image size will now be equal to 7x7x64.\n",
    "      Remember that during the convolution, we used stride 2 in the width and height directions.\n",
    "      Apply convolution operation, ReLU function and then dropout with probability equal to g_keep_prob \"\"\"\n",
    "#   z = tf.nn.conv2d_transpose(z, g_W2, [batch_size, 7, 7, 64], [1, 2, 2, 1], padding='SAME') # 7x7x64 image\n",
    "#   z = tf.nn.relu(z + g_b2)\n",
    "#   z = tf.nn.dropout(z, g_keep_prob)\n",
    "  z = # 7x7x64 image\n",
    "\n",
    "  \n",
    "  \"\"\" Create second transpose convolutional layer, please use SAME padding.\n",
    "      This layer will double the size of width and height of our image - \n",
    "      instead of 7x7x64, the image size will now be equal to 14x14x16.\n",
    "      Remember that during the convolution, we used stride 2 in the width and height directions.\n",
    "      Apply convolution operation, ReLU function and then dropout with probability equal to g_keep_prob \"\"\"\n",
    "#   z = tf.nn.conv2d_transpose(z, g_W3, [batch_size, 14, 14, 16], [1, 2, 2, 1], padding='SAME') # 14x14x16 image\n",
    "#   z = tf.nn.relu(z + g_b3)\n",
    "#   z = tf.nn.dropout(z, g_keep_prob)\n",
    "  z = # 14x14x16 image\n",
    "\n",
    "  \n",
    "  \"\"\" Create final transpose convolutional layer, please use SAME padding.\n",
    "      This layer will double the size of width and height of our image - \n",
    "      instead of 14x14x16, the image size will now be equal to 28x28x1.\n",
    "      Remember that during the convolution, we used stride 2 in the width and height directions.\n",
    "      Apply convolution operation, don't use ReLU function nor dropout as this is the final layer \"\"\"\n",
    "#   z = tf.nn.conv2d_transpose(z, g_W4, [batch_size, 28, 28, 1], [1, 2, 2, 1], padding='SAME') # 28x28x1 image\n",
    "  z = # 28x28x1 image\n",
    "\n",
    "  \n",
    "  \"\"\" Use the sigmoid function on the result of our transpose convolutions, to get the final image,\n",
    "      after applying sigmoid function we obtain values between 0 and 1, just as in the real MNIST samples \"\"\"\n",
    "#   G_prob = tf.nn.sigmoid(z + g_b4)\n",
    "  G_prob = \n",
    "\n",
    "  return G_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vllu9IYJVXUU"
   },
   "source": [
    "## GAN training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VHKO0DmlaVH4"
   },
   "source": [
    "We will now try to train our networks. For this purpose we have to define loss functions and optimizers for both Generator and Discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w80HXqGaNahn"
   },
   "source": [
    "#### Defining some auxiliary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "aKAL9HbRNaB3"
   },
   "outputs": [],
   "source": [
    "def sample_Z(m, n):\n",
    "  \"\"\"\n",
    "  Write a function, that will return the tf.Variable of shape [m, n], with\n",
    "  coefficients initialized by random, sampled from a normal distribution.\n",
    "  \"\"\"\n",
    "#   return np.random.normal(size=[m, n], scale = 1)\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k-9taCqLNiHP"
   },
   "source": [
    "#### Defining some constants and placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 89,
     "output_extras": [
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1490,
     "status": "ok",
     "timestamp": 1515374333225,
     "user": {
      "displayName": "Łukasz Maziarka",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "111922214428585581485"
     },
     "user_tz": -60
    },
    "id": "ac-jlqjTZYB7",
    "outputId": "91a68d48-ae17-4994-d6a4-d5c89307f566"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# nie uzywamy identyfikatora, bo nie musimy, jezeli trudniejsze obrazki, to ostatni wymiar powiinismy zwiekszyc o tyle razy ile \n",
    "# klas i wypelniamy 0 lub 1 w celu identyfikacji klasy\n",
    "\n",
    "# Constants\n",
    "batch_size = 64\n",
    "Z_dim = 100 # dimension of the random initialized variable, that will be used by the generator\n",
    "# wektor wejscia do generatora (szum z rnorm)\n",
    "\n",
    "# Discriminator \n",
    "X = tf.placeholder(tf.float32, shape=[None, 28, 28, 1]) # placeholder for the variable with the real MNIST images\n",
    "\n",
    "# Generator\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100]) # placeholder for the random initialized variable, that will be used by the generator\n",
    "g_keep_prob = 0.9 # dropout probability  # nie da sie ustawic ile zostanie zapomnianych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9juhqG0uaWUH"
   },
   "source": [
    "#### Generating the data by generator and calculating probabilities and logits by discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "y2Sz6tpbZYVa"
   },
   "outputs": [],
   "source": [
    "\"\"\" Use generator to generate the image based on the given random tensor Z \"\"\"\n",
    "# G_sample = generator(Z)\n",
    "G_sample = \n",
    "\n",
    "\n",
    "\"\"\" Use discriminator to get the probabilities and logits based on the real images X \"\"\"\n",
    "# D_real, D_logit_real = discriminator(X)\n",
    "D_real, D_logit_real = \n",
    "\n",
    "\n",
    "\"\"\" Use discriminator to get the probabilities and logits based on the generated images G_sample \"\"\"\n",
    "# D_fake, D_logit_fake = discriminator(G_sample)\n",
    "D_fake, D_logit_fake ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ViagDbPaW9b"
   },
   "source": [
    "#### Define the loss function and optimizer of the discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1B_A-G3fGNkd"
   },
   "source": [
    "The loss function of the discriminator consists of two parts. \n",
    "\n",
    "\n",
    "\n",
    "1.   First part tells us how our discriminator is good in classifying real examples. For this purpose we are calculating the loss function based on the true examples, comming from the real MNIST dataset. These probabilities should be as high as possible. \n",
    "2.   Second part tells us how our discriminator is good in classifying false, generated examples. For this purpose we are calculating the loss function based on the generated examples, comming from the generator. These probabilities should be as low as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "SY85VgcTZtYK"
   },
   "outputs": [],
   "source": [
    "eps = 1e-5\n",
    "\n",
    "\"\"\" Define the cross entropy loss function of our generator\n",
    "    Remember that the discriminator network returns probabilities and logits and during the training\n",
    "    we should use the second one, to overcome problems with numerical stability.\n",
    "    We should use now the function named softmax_cross_entropy_with_logits, that\n",
    "    is more numerical stable. \"\"\"\n",
    "\n",
    "\"\"\" Calculate the first part of discriminator loss function - loss based on the real examples \"\"\"\n",
    "# D_loss_real = tf.reduce_mean(-tf.log(D_real + eps))\n",
    "D_loss_real = \n",
    "\n",
    "\n",
    "\"\"\" Calculate the second part of discriminator loss function - loss based on the generated examples \"\"\"\n",
    "# D_loss_fake = tf.reduce_mean(-tf.log(1 - D_fake + eps))\n",
    "D_loss_fake = \n",
    "\n",
    "\n",
    "\"\"\" Loss of the discriminator network \"\"\"\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "\n",
    "\n",
    "\"\"\" Define the Adam optimizer with parameters equal to learning_rate=0.0002, beta1=0.5, that will minimize our cross_entropy loss function.\n",
    "    We also should use the var_list = [d_W1, d_W2, d_W3, d_W4, d_b1, d_b2, d_b3, d_b4] during the training\n",
    " #   it tells which weighst should be changed. Without that we would change weigts of generator for discriminator i vice versa\n",
    " \"\"\"\n",
    "# D_solver = tf.train.AdamOptimizer(0.0002, beta1=0.5).minimize(D_loss, var_list=[d_W1, d_W2, d_W3, d_W4, d_b1, d_b2, d_b3, d_b4])\n",
    "D_solver ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UGYxZnk1aXlJ"
   },
   "source": [
    "#### Define the loss function and optimizer of the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "mnD6f00wZYPB"
   },
   "outputs": [],
   "source": [
    "\"\"\" Define the cross entropy loss function of our generator\n",
    "    Remember that the discriminator network returns probabilities and logits and during the training\n",
    "    we should use the second one, to overcome problems with numerical stability.\n",
    "    We should use now the function named softmax_cross_entropy_with_logits, that\n",
    "    is more numerical stable. \"\"\"\n",
    "# G_loss = tf.reduce_mean(-tf.log(D_logit_fake + eps))\n",
    "G_loss = \n",
    "\n",
    "\n",
    "\"\"\" Define the Adam optimizer with parameters equal to learning_rate=0.0002, beta1=0.5, that will minimize our cross_entropy loss function.\n",
    "    We also should use the var_list = [g_W1, g_W2, g_W3, g_W4, g_b1, g_b2, g_b3, g_b4] during the training \"\"\"\n",
    "# G_solver = tf.train.AdamOptimizer(0.0002, beta1=0.5).minimize(G_loss, var_list=[g_W1, g_W2, g_W3, g_W4, g_b1, g_b2, g_b3, g_b4])\n",
    "G_solver ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6XbNEqi_aZQL"
   },
   "source": [
    "#### Training GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Et2a3H7JNn8"
   },
   "source": [
    "The following code will train our networks with using of our predefined Adam optimizers, based on the batch size equal to 64 and with 20000 steps.\n",
    "\n",
    "One can see how generator is learning how to create nice digits during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 888,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 260804,
     "status": "error",
     "timestamp": 1515374661934,
     "user": {
      "displayName": "Łukasz Maziarka",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "111922214428585581485"
     },
     "user_tz": -60
    },
    "id": "c9pFFS5CZ296",
    "outputId": "5f4fa544-3b40-4877-e6ec-337285f219f7"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-beae792a8a2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_loss_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD_solver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimages_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample_Z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_loss_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mG_solver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample_Z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mG_loss_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mG_loss_curr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())  # wywolanie wiecej niz 1 func - d_solver i d_loss\n",
    "  G_loss_epoch=0\n",
    "  D_loss_epoch=0\n",
    "\n",
    "  for i in range(1, 2000):\n",
    "    \n",
    "    images_batch = mnist.train.next_batch(batch_size)[0].reshape(-1,28,28,1)\n",
    "    \n",
    "    _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: images_batch, Z: sample_Z(batch_size, Z_dim)})\n",
    "    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(batch_size, Z_dim)})\n",
    "\n",
    "    G_loss_epoch += G_loss_curr\n",
    "    D_loss_epoch += D_loss_curr\n",
    "\n",
    "    if i % 750 == 1:\n",
    "\n",
    "      print('Epoch: {}'.format(i//750))\n",
    "      print('G_loss: {:.4}'.format(G_loss_epoch/750))\n",
    "      print('D loss: {:.4}'.format(D_loss_epoch/750))\n",
    "      \n",
    "      G_loss_epoch=0\n",
    "      D_loss_epoch=0\n",
    "\n",
    "      samples = sess.run(G_sample, feed_dict={Z: sample_Z(batch_size, Z_dim)})\n",
    "      for i in range(3):\n",
    "        plot(samples[i])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "GANs_answers.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
