{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "philipperemy.github.io/keras-stateful-lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomasz/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Input, LSTM, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(samples, features_no, ts_length):\n",
    "    arr = np.random.randn(samples, ts_length, features_no)\n",
    "    print(\"data shape {}\".format(arr.shape))\n",
    "    labels = np.random.randint(0,2 , size=(samples) )\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (1, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "samples = 1\n",
    "features_number = 1\n",
    "ts_length = 2\n",
    "\n",
    "data, labels = gen_data(samples,features_number,ts_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.26464407]\n",
      "  [-1.24543778]]]\n",
      "\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(\"\")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters number:\n",
    "\n",
    "* input 1, \n",
    "\n",
    "* hidden states 1\n",
    "\n",
    "1 parameter for matrices transforming input and hidden states (2 params) + 1 bias == 3 params\n",
    "\n",
    "Each gate has above paramaters (i,f,o,g) = 4*3 = 12 parameters for single LSTM node with single input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "    units = 1,\n",
    "#     input_shape=(None, ts_length, features_number) # number of examples (not the same as timesteps) is typically omitted in the input_shape arguments\n",
    "    input_shape=(1, 1)  # parameter can be ommited, input determined from data - examples below\n",
    "))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compile_param():\n",
    "    return {\"loss\": 'binary_crossentropy',\n",
    "              \"optimizer\": 'adam',\n",
    "              \"metrics\": ['accuracy']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (1, 1, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "    units = 1,\n",
    "))\n",
    "\n",
    "model.compile(**get_compile_param())\n",
    "\n",
    "samples = 1\n",
    "features_number = 1\n",
    "ts_length = 1\n",
    "\n",
    "data, labels = gen_data(samples,features_number,ts_length)\n",
    "\n",
    "model.fit(data, labels, epochs=1, verbose=0)\n",
    "\n",
    "model.summary()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (1, 2, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "    units = 1,\n",
    "))\n",
    "\n",
    "model.compile(**get_compile_param())\n",
    "\n",
    "samples = 1\n",
    "features_number = 1\n",
    "ts_length = 2  # time series length does not change LSTM cell\n",
    "\n",
    "data, labels = gen_data(samples,features_number,ts_length)\n",
    "\n",
    "model.fit(data, labels, epochs=1, verbose=0)\n",
    "\n",
    "model.summary()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (1, 1, 2)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 16\n",
      "Trainable params: 16\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "    units = 1,\n",
    "))\n",
    "\n",
    "model.compile(**get_compile_param())\n",
    "\n",
    "samples = 1\n",
    "features_number = 2  # + 1 parameter (1 additional (input dim)) for each matrix (4) transforming input data\n",
    "ts_length = 1\n",
    "\n",
    "data, labels = gen_data(samples,features_number,ts_length)\n",
    "\n",
    "model.fit(data, labels, epochs=1, verbose=0)\n",
    "\n",
    "model.summary()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (2, 1, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "    units = 1\n",
    "))\n",
    "\n",
    "model.compile(**get_compile_param())\n",
    "\n",
    "samples = 2   # number of sampels does not change LSTM cell\n",
    "features_number = 1\n",
    "ts_length = 1\n",
    "\n",
    "data, labels = gen_data(samples,features_number,ts_length)\n",
    "\n",
    "model.fit(data, labels, epochs=1, verbose=0)\n",
    "\n",
    "model.summary()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (1, 1, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 2)                 32        \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 35\n",
      "Trainable params: 35\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "    units = 2,\n",
    "))\n",
    "model.add(Dense(1))  # for compatibility of output dim (params: 2 weights and 1 bias)\n",
    "\n",
    "model.compile(**get_compile_param())\n",
    "\n",
    "samples = 1\n",
    "features_number = 1\n",
    "ts_length = 1\n",
    "\n",
    "data, labels = gen_data(samples,features_number,ts_length)\n",
    "\n",
    "model.fit(data, labels, epochs=1, verbose=0)\n",
    "\n",
    "model.summary()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.reddit.com/r/MachineLearning/comments/87djn7/d_what_is_meant_by_number_of_hidden_units_in_an/\n",
    "\n",
    "based on adam_jc answers:\n",
    "In more detail, there are 2 matrices (for transforming input and cell state) for 3 gates + cell state (one with dimensions n_input_features x n_units and one with n_units x n_units).\n",
    "\n",
    "For 2 units:\n",
    "\n",
    "so 8 matrices altogether in a cell. # of weights (not including bias term) = 4(n_input_features x n_units + n_units^2 )  (*)\n",
    "\n",
    "single unit, single feature: 4(1 * 1 + 1) + 4bias = 12\n",
    "\n",
    "single unit, two features: 4(2 * 1 + 1) + 4bias = 16\n",
    "\n",
    "two units, signle feature : 4(1 * 2 + 4) + 8bias = 32\n",
    "\n",
    "(Description of the equation (*)):\n",
    "\n",
    "(...)\n",
    "Through each gate we pass in our previous hidden state vector and our current timestep vector.\n",
    "\n",
    "So we have our previous hidden state vector with size n_units and our timestep vector with size n_input_features.\n",
    "\n",
    "The way I alluded to is why we have 2 matrices at each gate.\n",
    "\n",
    "One matrix has the size (n_input_features x n_units) which transforms our current timestep vector into a vector with size n_units\n",
    "\n",
    "The other matrix is size (n_units x n_units) which we use to transform our previous hidden state vector into a new vector, but still with size n_units  - each hidden state unit go through all athers hidden states units - that is why there is square\n",
    "\n",
    "Then we add these two resulting n_units vectors together with element-wise addition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM seq prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on \n",
    "\n",
    "https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/\n",
    "\n",
    "also very usefull \n",
    "\n",
    "https://datascience.stackexchange.com/questions/10836/the-difference-between-dense-and-timedistributeddense-of-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-to-one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 5)                 140       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 146\n",
      "Trainable params: 146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# prepare sequence\n",
    "length = 5\n",
    "seq = array([i/float(length) for i in range(length)])\n",
    "X = seq.reshape(len(seq), 1, 1)\n",
    "y = seq.reshape(len(seq), 1)\n",
    "# define LSTM configuration\n",
    "n_neurons = length\n",
    "n_batch = length\n",
    "n_epoch = 1000\n",
    "# create LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_neurons, input_shape=(1, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "print(model.summary())\n",
    "# train LSTM\n",
    "model.fit(X, y, epochs=n_epoch, batch_size=n_batch, verbose=0)\n",
    "# evaluate\n",
    "result = model.predict(X, batch_size=n_batch, verbose=0)\n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many-to-one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare sequence\n",
    "length = 5\n",
    "seq = array([i/float(length) for i in range(length)])\n",
    "X = seq.reshape(1, length, 1)\n",
    "y = seq.reshape(1, length)\n",
    "# define LSTM configuration\n",
    "n_neurons = length\n",
    "n_batch = 1\n",
    "n_epoch = 500\n",
    "# create LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_neurons, input_shape=(length, 1)))\n",
    "model.add(Dense(length))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "print(model.summary())\n",
    "# train LSTM\n",
    "model.fit(X, y, epochs=n_epoch, batch_size=n_batch, verbose=0)\n",
    "# evaluate\n",
    "result = model.predict(X, batch_size=n_batch, verbose=0)\n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many-to-many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare sequence\n",
    "length = 5\n",
    "seq = array([i/float(length) for i in range(length)])\n",
    "X = seq.reshape(1, length, 1)\n",
    "y = seq.reshape(1, length, 1)\n",
    "# define LSTM configuration\n",
    "n_neurons = 10\n",
    "n_batch = 1\n",
    "n_epoch = 1000\n",
    "# create LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_neurons, input_shape=(length, 1), return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "print(model.summary())\n",
    "# train LSTM\n",
    "model.fit(X, y, epochs=n_epoch, batch_size=n_batch, verbose=0)\n",
    "# evaluate\n",
    "result = model.predict(X, batch_size=n_batch, verbose=0)\n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TimeDistributed nak≈Çadane jest na wszystkie unity dabej sieci rekurencyjnej dla kazdego (pojedynczego) time stamp. Time stamp nie sa ze soba mieszane"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.6_tf",
   "language": "python",
   "name": "py3.6_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
